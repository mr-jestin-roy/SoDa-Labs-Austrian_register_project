<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      Vision Language Models for Historical Document Digitization - Monash
      Business School
    </title>
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&family=Roboto+Condensed:wght@700&display=swap"
      rel="stylesheet"
    />
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      :root {
        --monash-blue: #006dae;
        --monash-black: #000000;
        --monash-white: #ffffff;
        --monash-grey: #3c3c3c;
      }

      body {
        font-family: "Roboto", sans-serif;
        background: var(--monash-white);
        min-height: 100vh;
        padding: 40px 20px;
        color: var(--monash-black);
      }
      .presentation {
        max-width: 1000px;
        margin: 0 auto;
      }
      .slide {
        background: var(--monash-white);
        border-top: 6px solid var(--monash-blue);
        border-radius: 2px;
        padding: 60px 50px;
        margin-bottom: 40px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        page-break-after: always;
        min-height: 800px;
        display: flex;
        flex-direction: column;
        position: relative;
      }

      /* Cover Slide Special Styling */
      .slide.cover-slide {
        background: linear-gradient(
          135deg,
          var(--monash-blue) 0%,
          #004080 100%
        );
        color: var(--monash-white);
        border-top: none;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        text-align: center;
        padding: 60px 50px;
        position: relative;
      }

      .logo-header {
        position: absolute;
        top: 30px;
        left: 30px;
        display: flex;
        gap: 20px;
        align-items: center;
      }

      .monash-logo-top {
        width: 140px;
        height: auto;
        background: var(--monash-white);
        padding: 8px 10px;
        border-radius: 4px;
      }

      .monash-logo-top img {
        width: 100%;
        height: auto;
        display: block;
      }

      .soda-labs-logo-top {
        width: 50px;
        height: auto;
      }

      .soda-labs-logo-top img {
        width: 100%;
        height: auto;
        display: block;
      }

      .slide.cover-slide h1 {
        color: var(--monash-white);
        font-size: 2.6em;
        margin-bottom: 20px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
        margin-top: 40px;
      }

      .slide.cover-slide .subtitle {
        color: var(--monash-white);
        font-size: 1.3em;
        margin-bottom: 40px;
        opacity: 0.95;
      }

      .slide.cover-slide .author-info {
        color: var(--monash-white);
        font-size: 0.95em;
        margin-top: 40px;
      }

      .soda-labs-badge {
        display: inline-block;
        background: rgba(255, 255, 255, 0.15);
        color: var(--monash-white);
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 0.85em;
        margin-top: 30px;
        border: 1px solid rgba(255, 255, 255, 0.3);
        font-weight: 600;
      }

      /* Standard Slide Header */
      .slide h1 {
        color: var(--monash-blue);
        font-size: 2.2em;
        margin-bottom: 30px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
        border-bottom: 2px solid var(--monash-blue);
        padding-bottom: 15px;
      }
      .slide h2 {
        color: var(--monash-blue);
        font-size: 1.6em;
        margin-top: 30px;
        margin-bottom: 20px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
      }
      .slide h3 {
        color: var(--monash-black);
        font-size: 1.2em;
        margin-top: 20px;
        margin-bottom: 15px;
        font-weight: 600;
      }

      .intro-item {
        margin: 15px 0;
        padding: 12px 15px;
        background: #f0f7ff;
        border-left: 4px solid var(--monash-blue);
        border-radius: 2px;
      }
      .intro-item strong {
        color: var(--monash-blue);
      }

      .comparison-table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
        font-size: 0.95em;
      }
      .comparison-table th {
        background: var(--monash-blue);
        color: var(--monash-white);
        padding: 15px;
        text-align: left;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
      }
      .comparison-table td {
        padding: 12px 15px;
        border-bottom: 1px solid #ddd;
        background: #fafafa;
      }
      .comparison-table tr:hover {
        background: #f0f7ff;
      }
      .comparison-table .checkmark {
        color: #00a651;
        font-weight: bold;
      }
      .comparison-table .cross {
        color: #d32f2f;
        font-weight: bold;
      }
      .comparison-table .trade-off {
        color: #ff9800;
        font-weight: bold;
      }

      .metrics-grid {
        display: grid;
        grid-template-columns: 1fr 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
      }
      .metric-card {
        background: var(--monash-blue);
        color: var(--monash-white);
        padding: 25px;
        border-radius: 4px;
        text-align: center;
        box-shadow: 0 2px 6px rgba(0, 109, 174, 0.2);
      }
      .metric-value {
        font-size: 2.5em;
        font-weight: 700;
        margin: 10px 0;
        font-family: "Roboto Condensed", sans-serif;
      }
      .metric-label {
        font-size: 0.95em;
        opacity: 0.95;
      }
      .advantage {
        color: #4ade80;
        font-weight: bold;
        margin-top: 10px;
      }

      .time-comparison-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
      }
      .time-card {
        padding: 25px;
        border-radius: 4px;
        text-align: center;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
      }
      .time-card.fast {
        background: #e8f5e9;
        border: 2px solid #00a651;
      }
      .time-card.slow {
        background: #fff3e0;
        border: 2px solid #ff9800;
      }
      .time-value {
        font-size: 2.2em;
        font-weight: 700;
        margin: 10px 0;
        font-family: "Roboto Condensed", sans-serif;
      }
      .time-label {
        font-size: 1em;
        font-weight: 600;
        margin-bottom: 8px;
      }
      .time-detail {
        font-size: 0.9em;
        opacity: 0.85;
        margin-top: 10px;
      }

      .discovery-box {
        background: #fffbf0;
        border-left: 4px solid #ff9800;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .discovery-title {
        color: #e65100;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .discovery-text {
        color: var(--monash-black);
        line-height: 1.6;
      }

      .recommendation-box {
        background: #f0f8f4;
        border-left: 4px solid #00a651;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .recommendation-title {
        color: #006b3f;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .recommendation-text {
        color: var(--monash-black);
        line-height: 1.6;
      }

      .tradeoff-box {
        background: #fff8e1;
        border-left: 4px solid #fbc02d;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .tradeoff-title {
        color: #f57f17;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .tradeoff-text {
        color: var(--monash-black);
        line-height: 1.6;
      }

      .phase-timeline {
        display: flex;
        justify-content: space-between;
        margin: 30px 0;
        position: relative;
      }
      .phase {
        flex: 1;
        text-align: center;
        padding: 20px;
      }
      .phase-circle {
        width: 80px;
        height: 80px;
        background: var(--monash-blue);
        border-radius: 50%;
        margin: 0 auto 10px;
        display: flex;
        align-items: center;
        justify-content: center;
        color: var(--monash-white);
        font-weight: bold;
        font-size: 1.3em;
        box-shadow: 0 2px 6px rgba(0, 109, 174, 0.2);
      }
      .phase-title {
        font-weight: 600;
        color: var(--monash-black);
        margin-bottom: 10px;
      }
      .phase-text {
        font-size: 0.9em;
        color: var(--monash-grey);
      }

      .conclusion-highlight {
        background: var(--monash-blue);
        color: var(--monash-white);
        padding: 30px;
        border-radius: 4px;
        margin: 20px 0;
        font-size: 1.05em;
        line-height: 1.8;
        box-shadow: 0 2px 6px rgba(0, 109, 174, 0.2);
      }

      .page-break {
        page-break-after: always;
      }

      .footer {
        text-align: center;
        color: var(--monash-grey);
        font-size: 0.8em;
        margin-top: 50px;
        padding-top: 20px;
        border-top: 1px solid #ddd;
        display: flex;
        justify-content: space-between;
        align-items: center;
      }
      .footer-left {
        text-align: left;
        display: flex;
        align-items: center;
        gap: 10px;
      }
      .footer-logo {
        width: 100px;
        height: auto;
      }
      .footer-logo img {
        width: 100px;
        height: auto;
      }
      .footer-right {
        text-align: right;
      }

      .project-overview {
        background: #f0f7ff;
        border: 2px solid var(--monash-blue);
        border-radius: 4px;
        padding: 25px;
        margin: 20px 0;
      }
      .project-overview h3 {
        color: var(--monash-blue);
        margin-bottom: 15px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
      }
      .overview-item {
        margin: 15px 0;
        padding: 12px;
        background: var(--monash-white);
        border-left: 4px solid var(--monash-blue);
        border-radius: 2px;
      }
      .overview-label {
        color: var(--monash-blue);
        font-weight: 700;
      }

      .sub-aim-card-collapsed {
        background: #f0f7ff;
        border: 2px solid var(--monash-blue);
        border-radius: 4px;
        padding: 20px;
        margin: 15px 0;
      }
      .sub-aim-number {
        background: var(--monash-blue);
        color: var(--monash-white);
        width: 40px;
        height: 40px;
        border-radius: 50%;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        font-weight: bold;
        font-size: 1.2em;
        margin-right: 15px;
        vertical-align: top;
      }
      .sub-aim-header {
        display: flex;
        align-items: flex-start;
        gap: 15px;
        margin-bottom: 12px;
      }
      .sub-aim-title {
        color: var(--monash-black);
        font-weight: 600;
        font-size: 1.05em;
        flex: 1;
      }
      .sub-aim-text {
        color: var(--monash-black);
        font-size: 0.95em;
        line-height: 1.6;
      }

      .reference-images {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
        background: #fafafa;
        padding: 20px;
        border-radius: 4px;
        border: 1px solid #ddd;
      }
      .image-card {
        text-align: center;
        background: var(--monash-white);
        padding: 15px;
        border-radius: 4px;
        box-shadow: 0 1px 4px rgba(0, 0, 0, 0.1);
      }
      .image-card img {
        max-width: 100%;
        height: auto;
        border-radius: 3px;
        margin-bottom: 12px;
        border: 1px solid #ddd;
      }
      .image-caption {
        font-size: 0.9em;
        color: var(--monash-black);
        line-height: 1.5;
        text-align: left;
      }
      .image-label {
        font-weight: 700;
        color: var(--monash-blue);
        margin-bottom: 5px;
        font-size: 0.95em;
      }
      .image-details {
        font-size: 0.85em;
        color: var(--monash-grey);
        margin-top: 8px;
        font-style: italic;
      }

      ul,
      ol {
        margin-left: 20px;
        line-height: 1.8;
      }
      li {
        margin-bottom: 8px;
      }
    </style>
  </head>
  <body>
    <div class="presentation">
      <!-- COVER SLIDE -->
      <div class="slide cover-slide">
        <div class="logo-header">
          <div class="monash-logo-top">
            <img src="./assets/monash-logo.png" alt="Monash University Logo" />
          </div>
          <div class="soda-labs-logo-top">
            <img src="./assets/soda-labs-logo.jpeg" alt="SoDa Labs Logo" />
          </div>
        </div>

        <div style="text-align: center; margin-top: 40px">
          <h1>Vision Language Models for Historical Document Digitization</h1>
          <p class="subtitle">
            Benchmarking Qwen3-VL Against Traditional OCR for Austrian Parish
            Records
          </p>
          <div class="author-info">
            <p><strong>Jestin Roy</strong></p>
            <p>Research Assistant, Department of Economics</p>
            <p>Master of Data Science Student</p>
            <p>Monash Business School, Monash University</p>
            <p style="margin-top: 20px; opacity: 0.85">
              June 2025 - Present | Winter Research Scholarship Program
            </p>
          </div>
          <div class="soda-labs-badge">üöÄ SoDa Labs Research Initiative</div>
        </div>
      </div>

      <!-- SLIDE 1: Research Background & Motivation -->
      <div class="slide">
        <h1>Research Background & Motivation</h1>

        <h2>Personal Introduction</h2>
        <div class="intro-item"><strong>Name:</strong> Jestin Roy</div>
        <div class="intro-item">
          <strong>Position:</strong> Research Assistant (RA) under Professor
          Paul, Department of Economics
        </div>
        <div class="intro-item">
          <strong>Education:</strong> Master of Data Science student
        </div>
        <div class="intro-item">
          <strong>Experience:</strong> AI Engineer & Full-Stack Developer (2
          years industrial experience)
        </div>
        <div class="intro-item">
          <strong>Project Duration:</strong> June 2025 - Present (Winter
          Research Scholarship Program)
        </div>

        <h2>The Research Challenge</h2>
        <div class="intro-item">
          <strong>Documents:</strong> Austrian parish records from 1800-1920s
          (Sterbb√ºcher [death registers], Taufe B√ºcher [baptism records])
        </div>
        <div class="intro-item">
          <strong>Script Type:</strong> S√ºtterlin/Kurrent script (historical
          German cursive and Gothic scripts)
        </div>
        <div class="intro-item">
          <strong>Research Goal:</strong> Accurate digitization for historical
          scholarship, genealogical research, and cultural heritage preservation
        </div>

        <h2>Why S√ºtterlin/Kurrent Scripts Are Challenging</h2>
        <ul>
          <li>
            <strong>S√ºtterlin (1911+):</strong> German Gothic cursive with sharp
            angles and connected letterforms
          </li>
          <li>
            <strong>Kurrent (1600s-1900s):</strong> Historical German script
            with unique character forms (≈ø, √§, √∂, √º variations)
          </li>
          <li>
            <strong>Visual Complexity:</strong> Ligatures, abbreviations, and
            archaic letterforms unfamiliar to standard ML models
          </li>
          <li>
            <strong>Training Data Gap:</strong> Modern OCR models trained on
            20th-21st century text, not 19th-century German scripts
          </li>
        </ul>

        <h2>The Problem: Predecessor Approach (trOCR + Danish Fine-Tuning)</h2>
        <div class="discovery-box">
          <div class="discovery-title">Why Character-Level OCR Failed</div>
          <div class="discovery-text">
            ‚ùå Language Mismatch: Trained on Danish, applied to German
            S√ºtterlin/Kurrent<br />
            ‚ùå Script Mismatch: Danish modern text ‚â† 19th-century Austrian
            Gothic scripts<br />
            ‚ùå No Visual Reasoning: Character-by-character OCR cannot understand
            document context<br />
            ‚ùå No Layout Understanding: Cannot distinguish table structure from
            continuous text<br />
            ‚ùå Limited Transfer: Extensive fine-tuning required for each new
            language/script
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">
            <div class="footer-logo">
              <img src="./assets/monash-logo.png" alt="Monash Logo" />
            </div>
            Monash Business School
          </div>
          <div class="footer-right">Slide 1 of 7</div>
        </div>
      </div>

      <!-- SLIDE 2: Methodological Breakthrough -->
      <div class="slide page-break">
        <h1>The Methodological Breakthrough</h1>

        <h2>Three Phases of the Research</h2>

        <div class="phase-timeline">
          <div class="phase">
            <div class="phase-circle">1</div>
            <div class="phase-title">Initial Baseline</div>
            <div class="phase-text">
              trOCR (Danish fine-tuning)<br />Poor generalization
            </div>
          </div>
          <div class="phase">
            <div class="phase-circle">2</div>
            <div class="phase-title">Ground Truth Creation</div>
            <div class="phase-text">
              Closed-Source LLMs (GPT-4.1)<br />Visual reasoning
            </div>
          </div>
          <div class="phase">
            <div class="phase-circle">3</div>
            <div class="phase-title">VLM Evaluation</div>
            <div class="phase-text">Qwen3-VL vs Chandra<br />Benchmarking</div>
          </div>
        </div>

        <h2>Why Closed-Source LLMs for Ground Truth?</h2>

        <table class="comparison-table">
          <tr>
            <th>Capability</th>
            <th>Fine-Tuning Approach</th>
            <th>Ground Truth Approach</th>
          </tr>
          <tr>
            <td><strong>Visual Reasoning</strong></td>
            <td class="cross">‚úó None (character-level only)</td>
            <td class="checkmark">‚úì Yes (holistic understanding)</td>
          </tr>
          <tr>
            <td><strong>Handwriting Context</strong></td>
            <td class="cross">‚úó Limited (language-specific)</td>
            <td class="checkmark">‚úì Full (visual + semantic)</td>
          </tr>
          <tr>
            <td><strong>Domain Adaptation Time</strong></td>
            <td>2-3 months</td>
            <td>Minutes</td>
          </tr>
          <tr>
            <td><strong>Accuracy on Historical Scripts</strong></td>
            <td>50-70%</td>
            <td>95%+ (human-level)</td>
          </tr>
          <tr>
            <td><strong>Cost to Scale</strong></td>
            <td>High (annotation labor)</td>
            <td>Low (same model)</td>
          </tr>
          <tr>
            <td><strong>Generalization Ability</strong></td>
            <td>Poor (language-specific)</td>
            <td>Excellent (cross-language)</td>
          </tr>
        </table>

        <h2>Current Investigation</h2>
        <div class="recommendation-box">
          <div class="recommendation-title">Research Question</div>
          <div class="recommendation-text">
            Can we achieve closed-source LLM performance using open-source
            Vision Language Models (VLMs) at lower cost and without fine-tuning?
          </div>
        </div>

        <h2>Models Being Evaluated</h2>
        <ul>
          <li>
            <strong>Qwen3-VL-32B:</strong> Open-source Vision Language Model
            trained on billions of image-text pairs
          </li>
          <li>
            <strong>Chandra OCR:</strong> Traditional specialized OCR engine
            (character detection and recognition)
          </li>
          <li>
            <strong>Against Ground Truth:</strong> GPT-4.1 and Gemini 2.5 Pro
            (visual reasoning, context understanding)
          </li>
          <li>
            <strong>Dataset:</strong> Austrian parish records (S√ºtterlin/Kurrent
            scripts, 1643-1920)
          </li>
        </ul>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 2 of 7</div>
        </div>
      </div>

      <!-- SLIDE 3: Benchmark Results with Reference Images -->
      <div class="slide page-break">
        <h1>Benchmark Results - Performance Metrics</h1>

        <h2>Reference Images: Proof-of-Concept Test Documents</h2>
        <div class="reference-images">
          <div class="image-card">
            <img
              src="./assets/Althofen_Sterbbuch_TomVII_1905-1920_page_006.jpg"
              alt="Modern Austrian Sterbbuch 1905-1920"
            />
            <div class="image-caption">
              <div class="image-label">
                Althofen_Sterbbuch_TomVII_1905-1920_page_006.jpg
              </div>
              <strong>Era:</strong> Modern (1905-1920)<br />
              <strong>Type:</strong> Sterbbuch (Death Register)<br />
              <strong>Performance:</strong> 81.27% Word F1 ‚úì
              <div class="image-details">Production-Ready Accuracy</div>
            </div>
          </div>

          <div class="image-card">
            <img
              src="./assets/Wein_Neiderosterich_MariaBrunn_1887_TaufeBuch_0012.jpg"
              alt="Late 19th Century Austrian Taufebuch 1887"
            />
            <div class="image-caption">
              <div class="image-label">
                Wein_Neiderosterich_MariaBrunn_1887_TaufeBuch_0012.jpg
              </div>
              <strong>Era:</strong> Late 19th Century (1887)<br />
              <strong>Type:</strong> Taufebuch (Baptism Register)<br />
              <strong>Performance:</strong> 61.54% Word F1 ‚ö†
              <div class="image-details">Usable + Manual Verification</div>
            </div>
          </div>

          <div class="image-card">
            <img
              src="./assets/Vienna_1783_StPeter_TaufeBuch_0017.jpg"
              alt="18th Century Viennese Taufebuch 1783"
            />
            <div class="image-caption">
              <div class="image-label">
                Vienna_1783_StPeter_TaufeBuch_0017.jpg
              </div>
              <strong>Era:</strong> 18th Century (1783)<br />
              <strong>Type:</strong> Taufebuch (Baptism Register)<br />
              <strong>Performance:</strong> 37.01% Word F1 ‚ö†
              <div class="image-details">
                UTF-8 Fix: 0.92% ‚Üí 37.01% (40x improvement)
              </div>
            </div>
          </div>

          <div class="image-card">
            <img
              src="./assets/Althofen_SterbBuch_1643_A05_022-1_00007.jpg"
              alt="17th Century Pure Kurrent Script 1643"
            />
            <div class="image-caption">
              <div class="image-label">
                Althofen_SterbBuch_1643_A05_022-1_00007.jpg
              </div>
              <strong>Era:</strong> 17th Century (1643)<br />
              <strong>Type:</strong> Sterbbuch (Death Register)<br />
              <strong>Performance:</strong> 11.62% Word F1 ‚úó
              <div class="image-details">
                Pure Kurrent - Not Viable Without Specialized Training
              </div>
            </div>
          </div>
        </div>

        <h2>Overall Performance Comparison</h2>

        <div class="metrics-grid">
          <div class="metric-card">
            <div class="metric-label">Word F1 Score</div>
            <div class="metric-value">84.96%</div>
            <div class="metric-label">Qwen3-VL</div>
            <div class="advantage">+4.44% vs Chandra</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Word Recall</div>
            <div class="metric-value">86.81%</div>
            <div class="metric-label">Qwen3-VL</div>
            <div class="advantage">+8.61% vs Chandra</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Character F1</div>
            <div class="metric-value">97.49%</div>
            <div class="metric-label">Qwen3-VL</div>
            <div class="advantage">+0.90% vs Chandra</div>
          </div>
        </div>

        <h2>Key Insight: The Character-Layout Gap</h2>
        <div class="discovery-box">
          <div class="discovery-title">Critical Finding</div>
          <div class="discovery-text">
            Both models achieve 97% character-level accuracy, but only 85%
            word-level accuracy.<br /><br />
            <strong>Implication:</strong> Layout understanding (not character
            recognition) is the primary bottleneck.<br />
            Models successfully read individual characters but struggle to
            preserve word boundaries and table structure.
          </div>
        </div>

        <h2>Performance by Document Era (Temporal Trend)</h2>

        <table class="comparison-table">
          <tr>
            <th>Era</th>
            <th>Period</th>
            <th>Qwen3-VL Word F1</th>
            <th>Status</th>
          </tr>
          <tr>
            <td><strong>Modern</strong></td>
            <td>1905-1920</td>
            <td>81.27%</td>
            <td class="checkmark">‚úì Production-Ready</td>
          </tr>
          <tr>
            <td><strong>Late 19th Century</strong></td>
            <td>1887</td>
            <td>61.54%</td>
            <td>‚ö† Usable + Verification</td>
          </tr>
          <tr>
            <td><strong>18th Century</strong></td>
            <td>1783</td>
            <td>37.01%</td>
            <td>‚ö† High Manual Effort</td>
          </tr>
          <tr>
            <td><strong>17th Century</strong></td>
            <td>1643</td>
            <td>11.62%</td>
            <td class="cross">‚úó Not Viable</td>
          </tr>
        </table>

        <h2>Critical Discovery: UTF-8 Encoding Impact</h2>
        <div class="discovery-box">
          <div class="discovery-title">
            Vienna1783 Case Study - 40x Improvement
          </div>
          <div class="discovery-text">
            <strong>Before proper UTF-8 encoding:</strong> 0.92% Word F1
            (catastrophic)<br />
            <strong>After UTF-8 fix:</strong> 37.01% Word F1 (restored
            usability)<br /><br />
            <strong>Lesson:</strong> Data quality validation is as important as
            model selection. Character encoding errors can destroy benchmark
            reliability.
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 3 of 7</div>
        </div>
      </div>

      <!-- SLIDE 4: Computational Trade-Offs NEW -->
      <div class="slide page-break">
        <h1>Critical Trade-Off: Accuracy vs. Computational Time</h1>

        <h2>Processing Time Comparison</h2>
        <div class="time-comparison-grid">
          <div class="time-card slow">
            <div class="time-label">Qwen3-VL-32B</div>
            <div class="time-value">13 min</div>
            <div class="time-detail">per test image<br />(Single run)</div>
          </div>
          <div class="time-card fast">
            <div class="time-label">Chandra OCR</div>
            <div class="time-value">2-3 min</div>
            <div class="time-detail">per test image<br />(5-6x faster)</div>
          </div>
        </div>

        <h2>Scalability Impact Analysis</h2>

        <table class="comparison-table">
          <tr>
            <th>Metric</th>
            <th>Qwen3-VL</th>
            <th>Chandra OCR</th>
          </tr>
          <tr>
            <td><strong>Time per Image</strong></td>
            <td>13 minutes</td>
            <td>2-3 minutes</td>
          </tr>
          <tr>
            <td><strong>Time Cost Ratio</strong></td>
            <td class="trade-off">‚ö† 5-6x slower</td>
            <td class="checkmark">‚úì Baseline (faster)</td>
          </tr>
          <tr>
            <td><strong>1000 Images</strong></td>
            <td>~9 days (216 hours)</td>
            <td>~2 days (33-50 hours)</td>
          </tr>
          <tr>
            <td><strong>10,000 Images</strong></td>
            <td>~90 days (2,166 hours)</td>
            <td>~21 days (330-500 hours)</td>
          </tr>
          <tr>
            <td><strong>Word F1 Accuracy</strong></td>
            <td class="checkmark">84.96% (Superior)</td>
            <td>80.52% (Baseline)</td>
          </tr>
          <tr>
            <td><strong>Accuracy per Extra Minute</strong></td>
            <td class="checkmark">+0.44% gain / 11 min</td>
            <td class="cross">‚úó Lower baseline</td>
          </tr>
        </table>

        <h2>Computational Considerations</h2>
        <div class="tradeoff-box">
          <div class="tradeoff-title">
            ‚öñÔ∏è Trade-Off Analysis: When to Use Each Model
          </div>
          <div class="tradeoff-text">
            <strong>Use Qwen3-VL when:</strong><br />
            ‚úì Accuracy is critical (historical preservation, legal/genealogical
            records)<br />
            ‚úì Small-to-medium archive size (&lt; 5,000 images)<br />
            ‚úì Can leverage batch processing on HPC clusters (parallelization)<br />
            ‚úì Need layout preservation for tables &amp; structured data<br /><br />
            <strong>Use Chandra when:</strong><br />
            ‚úì Speed is priority (rapid scanning/digitization preview)<br />
            ‚úì Large archives requiring fast turnaround (&gt; 20,000 images)<br />
            ‚úì Limited computational resources<br />
            ‚úì Can tolerate 4% accuracy loss for 5-6x speed gain
          </div>
        </div>

        <h2>Parallelization Strategy for Large-Scale Deployment</h2>
        <div class="discovery-box">
          <div class="discovery-title">
            HPC Optimization: Mitigating Time Trade-Off
          </div>
          <div class="discovery-text">
            <strong>Current Constraint:</strong> 13 min per image (single
            GPU)<br /><br />
            <strong>Solution via Nectar HPC:</strong><br />
            ‚Ä¢ Deploy 10x parallel GPU workers ‚Üí Process 10 images
            simultaneously<br />
            ‚Ä¢ Total time: ~1.3 min per image batch<br />
            ‚Ä¢ Process 1,000 images in ~22 hours (vs. 216 hours serial)<br />
            ‚Ä¢ Cost-benefit: 84.96% accuracy justifies longer wait time when
            parallelized
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 4 of 7</div>
        </div>
      </div>

      <!-- SLIDE 5: Key Discoveries & Recommendations -->
      <div class="slide page-break">
        <h1>Key Discoveries & Recommendations</h1>

        <h2>Research Question & Answer</h2>
        <div class="conclusion-highlight">
          <strong
            >Q: Can Vision Language Models outperform traditional OCR on
            historical Austrian S√ºtterlin/Kurrent manuscripts without
            fine-tuning?</strong
          ><br /><br />
          <strong>A: YES ‚úì (With Computational Trade-Off)</strong><br /><br />
          Evidence: Qwen3-VL achieved 84.96% Word F1 with zero training on
          German data, a 5.5% improvement over traditional OCR baseline (Chandra
          at 80.52%), though with 5-6x longer processing time.
        </div>

        <h2>Four Key Discoveries</h2>

        <div class="discovery-box">
          <div class="discovery-title">1. Character Recognition is Solved</div>
          <div class="discovery-text">
            Both Qwen3-VL (97.49%) and Chandra (96.59%) achieve near-perfect
            character-level accuracy. Investment in character recognition has
            diminishing returns. Focus should shift to layout understanding.
          </div>
        </div>

        <div class="discovery-box">
          <div class="discovery-title">2. Document Age is the Key Driver</div>
          <div class="discovery-text">
            Performance directly correlates with document era: Modern (81%) ‚Üí
            19th century (61%) ‚Üí 18th century (37%) ‚Üí 17th century (12%). Pure
            Kurrent/Fraktur scripts require specialized training.
          </div>
        </div>

        <div class="discovery-box">
          <div class="discovery-title">
            3. Data Quality Beats Algorithm Sophistication
          </div>
          <div class="discovery-text">
            UTF-8 encoding fix provided 40x improvement. Proper data
            preprocessing and validation must precede benchmarking. One encoding
            error invalidated all downstream analysis.
          </div>
        </div>

        <div class="discovery-box">
          <div class="discovery-title">
            4. VLMs Win Without Fine-Tuning (Accuracy-First Approach)
          </div>
          <div class="discovery-text">
            Qwen3-VL's 4.44 percentage point advantage with ZERO training on
            German data demonstrates foundation model approach superior to
            specialized OCR, despite longer processing time.
          </div>
        </div>

        <h2>Immediate Recommendations (Next 3-6 Months)</h2>

        <div class="recommendation-box">
          <div class="recommendation-title">
            1. Deploy Qwen3-VL with HPC Parallelization
          </div>
          <div class="recommendation-text">
            ‚úì Set up batch processing pipeline on Nectar HPC with 10x GPU
            workers<br />
            ‚úì Process Austrian documents 1850+ in parallel (effective ~1.3 min
            per image)<br />
            ‚úì Expected accuracy: 61-81% Word F1<br />
            ‚úì Acceptable for archives valuing accuracy over speed
          </div>
        </div>

        <div class="recommendation-box">
          <div class="recommendation-title">
            2. Hybrid Strategy for Large Archives
          </div>
          <div class="recommendation-text">
            ‚úì Use Chandra for initial rapid scanning/preview (5-6x faster)<br />
            ‚úì Apply Qwen3-VL to high-priority/sensitive documents only<br />
            ‚úì Implement quality-based flagging: Route F1 &lt; 70% to Qwen3-VL
            refinement
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 5 of 7</div>
        </div>
      </div>

      <!-- SLIDE 6: Broader Research Context -->
      <div class="slide page-break">
        <h1>Broader Research Context & Project Aims</h1>

        <h2>The Bigger Picture: Historical Economic & Demographic Analysis</h2>
        <div class="project-overview">
          <h3>Project Aim</h3>
          <div class="overview-item">
            <span class="overview-label">Goal:</span> Perform layout analysis
            and handwritten text recognition (HTR) on Austrian parish records,
            extract tabular data, identify key figures from regions, and track
            their descendants' economic progress across decades (1800-1920s,
            including World War I and beyond).
          </div>
          <div class="overview-item">
            <span class="overview-label">Historical Context:</span> Tracing
            families' economic trajectories during transformative periods in
            Austrian history
          </div>
        </div>

        <h2>The Six-Stage Pipeline</h2>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">1</div>
            <div class="sub-aim-title">
              Ground Truth Creation: Hand-Transcribe Austrian Birth Certificates
            </div>
          </div>
          <div class="sub-aim-text">
            Using Transkribus desktop app, transcribe 496 word-level images from
            Austrian birth certificates. Dataset manually refined by Prof. Paul
            using PyLaia HTR model (German_Kurrent_17th-18th, CER: 6.00% train /
            5.50% validation).
          </div>
        </div>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">2-4</div>
            <div class="sub-aim-title">
              HTR Model Training & Testing ‚Üí Entity Extraction
            </div>
          </div>
          <div class="sub-aim-text">
            <strong>Sub-aim 2:</strong> Train HTR model (TrOCR) on combined
            open-source HANA + hand-transcribed Austrian data.<br />
            <strong>Sub-aim 3:</strong> Test trained model on complete Austrian
            birth dataset, producing structured machine-readable
            transcriptions.<br />
            <strong>Sub-aim 4:</strong> Apply Named Entity Recognition (NER) to
            extract names, professions, and place names from HTR outputs,
            enabling demographic mapping and occupational analysis.
          </div>
        </div>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">5</div>
            <div class="sub-aim-title">
              Construct Historical Social Networks
            </div>
          </div>
          <div class="sub-aim-text">
            Map relationships (family ties, godparent relationships, community
            associations) from birth, marriage, and death registers. Create
            connection graphs revealing social connectivity patterns and
            enabling lineage tracing across generations.
          </div>
        </div>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">6</div>
            <div class="sub-aim-title">Demographic & Longitudinal Analysis</div>
          </div>
          <div class="sub-aim-text">
            Conduct demographic analysis examining clusters by profession,
            geography, and social connectivity. Link to historical events (WWI,
            radical movements) to trace economic progress, social status shifts,
            and the impact of historical events on community development.
          </div>
        </div>

        <h2>How Qwen3-VL Enables the Pipeline</h2>
        <div class="conclusion-highlight">
          Qwen3-VL's superior layout understanding and handwriting recognition
          (84.96% Word F1) directly enables high-quality transcriptions in
          Sub-aims 2-3, serving as the foundation for all downstream entity
          extraction (Sub-aim 4), network construction (Sub-aim 5), and
          historical analysis (Sub-aim 6).
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 6 of 7</div>
        </div>
      </div>

      <!-- CONCLUSION SLIDE -->
      <div class="slide page-break">
        <h1>Conclusion: Paradigm Shift + Practical Deployment</h1>

        <div class="conclusion-highlight" style="margin-top: 30px">
          <strong>From:</strong> Fine-tuning external datasets on
          language-specific, character-sequence models<br /><br />
          <strong>To:</strong> Using foundation models with semantic
          understanding and zero domain-specific training<br /><br />
          <strong>Reality:</strong> 84.96% accuracy (superior), 13 min per image
          (slower), but parallelizable on HPC
        </div>

        <h2 style="margin-top: 50px">Impact Summary</h2>

        <div class="discovery-box">
          <div class="discovery-title">Research Contribution</div>
          <div class="discovery-text">
            ‚úì Demonstrated VLMs outperform traditional OCR on historical
            handwriting<br />
            ‚úì Proved ground-truth approach superior to external dataset
            fine-tuning<br />
            ‚úì Identified layout understanding as key bottleneck (not character
            recognition)<br />
            ‚úì Validated UTF-8 encoding importance in OCR pipelines<br />
            ‚úì
            <strong
              >Quantified computational trade-off (5-6x slower, 4.44% more
              accurate)</strong
            >
          </div>
        </div>

        <div class="recommendation-box">
          <div class="recommendation-title">
            Next Phase: Implementation (SoDa Labs Initiative)
          </div>
          <div class="recommendation-text">
            ‚úì Deploy Qwen3-VL with HPC parallelization for Austrian parish
            record digitization<br />
            ‚úì Optional fine-tuning on ground-truth dataset for incremental
            improvements<br />
            ‚úì Establish reproducible methodology for historical document OCR
            with time/accuracy trade-offs<br />
            ‚úì Scale to other European archives (Czech, Polish, Hungarian
            records)
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">
            Monash Business School, Monash University
          </div>
          <div class="footer-right">Slide 7 of 7</div>
        </div>
      </div>
    </div>
  </body>
</html>

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>
      Vision Language Models for Historical Document Digitization - Monash
      Business School
    </title>
    <link
      href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;500;700&family=Roboto+Condensed:wght@700&display=swap"
      rel="stylesheet"
    />
    <style>
      * {
        margin: 0;
        padding: 0;
        box-sizing: border-box;
      }

      :root {
        --monash-blue: #006dae;
        --monash-black: #000000;
        --monash-white: #ffffff;
        --monash-grey: #3c3c3c;
      }

      body {
        font-family: "Roboto", sans-serif;
        background: var(--monash-white);
        min-height: 100vh;
        padding: 40px 20px;
        color: var(--monash-black);
      }
      .presentation {
        max-width: 1000px;
        margin: 0 auto;
      }
      .slide {
        background: var(--monash-white);
        border-top: 6px solid var(--monash-blue);
        border-radius: 2px;
        padding: 60px 50px;
        margin-bottom: 40px;
        box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
        page-break-after: always;
        min-height: 800px;
        display: flex;
        flex-direction: column;
        position: relative;
      }

      /* Cover Slide Special Styling */
      .slide.cover-slide {
        background: linear-gradient(
          135deg,
          var(--monash-blue) 0%,
          #004080 100%
        );
        color: var(--monash-white);
        border-top: none;
        display: flex;
        flex-direction: column;
        justify-content: center;
        align-items: center;
        text-align: center;
        padding: 60px 50px;
        position: relative;
      }

      .logo-header {
        position: absolute;
        top: 30px;
        left: 30px;
        display: flex;
        gap: 20px;
        align-items: center;
      }

      .monash-logo-top {
        width: 140px;
        height: auto;
        background: var(--monash-white);
        padding: 8px 10px;
        border-radius: 4px;
      }

      .monash-logo-top img {
        width: 100%;
        height: auto;
        display: block;
      }

      .soda-labs-logo-top {
        width: 50px;
        height: auto;
      }

      .soda-labs-logo-top img {
        width: 100%;
        height: auto;
        display: block;
      }

      .slide.cover-slide h1 {
        color: var(--monash-white);
        font-size: 2.6em;
        margin-bottom: 20px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
        margin-top: 40px;
      }

      .slide.cover-slide .subtitle {
        color: var(--monash-white);
        font-size: 1.3em;
        margin-bottom: 40px;
        opacity: 0.95;
      }

      .slide.cover-slide .author-info {
        color: var(--monash-white);
        font-size: 0.95em;
        margin-top: 40px;
      }

      .soda-labs-badge {
        display: inline-block;
        background: rgba(255, 255, 255, 0.15);
        color: var(--monash-white);
        padding: 8px 16px;
        border-radius: 20px;
        font-size: 0.85em;
        margin-top: 30px;
        border: 1px solid rgba(255, 255, 255, 0.3);
        font-weight: 600;
      }

      /* Standard Slide Header */
      .slide h1 {
        color: var(--monash-blue);
        font-size: 2.2em;
        margin-bottom: 30px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
        border-bottom: 2px solid var(--monash-blue);
        padding-bottom: 15px;
      }
      .slide h2 {
        color: var(--monash-blue);
        font-size: 1.6em;
        margin-top: 30px;
        margin-bottom: 20px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
      }
      .slide h3 {
        color: var(--monash-black);
        font-size: 1.2em;
        margin-top: 20px;
        margin-bottom: 15px;
        font-weight: 600;
      }

      .intro-item {
        margin: 15px 0;
        padding: 12px 15px;
        background: #f0f7ff;
        border-left: 4px solid var(--monash-blue);
        border-radius: 2px;
      }
      .intro-item strong {
        color: var(--monash-blue);
      }

      .comparison-table {
        width: 100%;
        border-collapse: collapse;
        margin: 20px 0;
        font-size: 0.95em;
      }
      .comparison-table th {
        background: var(--monash-blue);
        color: var(--monash-white);
        padding: 15px;
        text-align: left;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
      }
      .comparison-table td {
        padding: 12px 15px;
        border-bottom: 1px solid #ddd;
        background: #fafafa;
      }
      .comparison-table tr:hover {
        background: #f0f7ff;
      }
      .comparison-table .checkmark {
        color: #00a651;
        font-weight: bold;
      }
      .comparison-table .cross {
        color: #d32f2f;
        font-weight: bold;
      }
      .comparison-table .trade-off {
        color: #ff9800;
        font-weight: bold;
      }

      .metrics-grid {
        display: grid;
        grid-template-columns: 1fr 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
      }
      .metric-card {
        background: var(--monash-blue);
        color: var(--monash-white);
        padding: 25px;
        border-radius: 4px;
        text-align: center;
        box-shadow: 0 2px 6px rgba(0, 109, 174, 0.2);
      }
      .metric-value {
        font-size: 2.5em;
        font-weight: 700;
        margin: 10px 0;
        font-family: "Roboto Condensed", sans-serif;
      }
      .metric-label {
        font-size: 0.95em;
        opacity: 0.95;
      }
      .advantage {
        color: #4ade80;
        font-weight: bold;
        margin-top: 10px;
      }

      .time-comparison-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
      }
      .time-card {
        padding: 25px;
        border-radius: 4px;
        text-align: center;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
      }
      .time-card.fast {
        background: #e8f5e9;
        border: 2px solid #00a651;
      }
      .time-card.slow {
        background: #fff3e0;
        border: 2px solid #ff9800;
      }
      .time-value {
        font-size: 2.2em;
        font-weight: 700;
        margin: 10px 0;
        font-family: "Roboto Condensed", sans-serif;
      }
      .time-label {
        font-size: 1em;
        font-weight: 600;
        margin-bottom: 8px;
      }
      .time-detail {
        font-size: 0.9em;
        opacity: 0.85;
        margin-top: 10px;
      }

      .discovery-box {
        background: #fffbf0;
        border-left: 4px solid #ff9800;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .discovery-title {
        color: #e65100;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .discovery-text {
        color: var(--monash-black);
        line-height: 1.6;
      }

      .recommendation-box {
        background: #f0f8f4;
        border-left: 4px solid #00a651;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .recommendation-title {
        color: #006b3f;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .recommendation-text {
        color: var(--monash-black);
        line-height: 1.6;
      }

      .tradeoff-box {
        background: #fff8e1;
        border-left: 4px solid #fbc02d;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .tradeoff-title {
        color: #f57f17;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .tradeoff-text {
        color: var(--monash-black);
        line-height: 1.6;
      }

      .phase-timeline {
        display: flex;
        justify-content: space-between;
        margin: 30px 0;
        position: relative;
      }
      .phase {
        flex: 1;
        text-align: center;
        padding: 20px;
      }
      .phase-circle {
        width: 80px;
        height: 80px;
        background: var(--monash-blue);
        border-radius: 50%;
        margin: 0 auto 10px;
        display: flex;
        align-items: center;
        justify-content: center;
        color: var(--monash-white);
        font-weight: bold;
        font-size: 1.3em;
        box-shadow: 0 2px 6px rgba(0, 109, 174, 0.2);
      }
      .phase-title {
        font-weight: 600;
        color: var(--monash-black);
        margin-bottom: 10px;
      }
      .phase-text {
        font-size: 0.9em;
        color: var(--monash-grey);
      }

      .conclusion-highlight {
        background: var(--monash-blue);
        color: var(--monash-white);
        padding: 30px;
        border-radius: 4px;
        margin: 20px 0;
        font-size: 1.05em;
        line-height: 1.8;
        box-shadow: 0 2px 6px rgba(0, 109, 174, 0.2);
      }

      .page-break {
        page-break-after: always;
      }

      .footer {
        text-align: center;
        color: var(--monash-grey);
        font-size: 0.8em;
        margin-top: 50px;
        padding-top: 20px;
        border-top: 1px solid #ddd;
        display: flex;
        justify-content: space-between;
        align-items: center;
      }
      .footer-left {
        text-align: left;
        display: flex;
        align-items: center;
        gap: 10px;
      }
      .footer-logo {
        width: 100px;
        height: auto;
      }
      .footer-logo img {
        width: 100px;
        height: auto;
      }
      .footer-right {
        text-align: right;
      }

      .project-overview {
        background: #f0f7ff;
        border: 2px solid var(--monash-blue);
        border-radius: 4px;
        padding: 25px;
        margin: 20px 0;
      }
      .project-overview h3 {
        color: var(--monash-blue);
        margin-bottom: 15px;
        font-family: "Roboto Condensed", sans-serif;
        font-weight: 700;
      }
      .overview-item {
        margin: 15px 0;
        padding: 12px;
        background: var(--monash-white);
        border-left: 4px solid var(--monash-blue);
        border-radius: 2px;
      }
      .overview-label {
        color: var(--monash-blue);
        font-weight: 700;
      }

      .sub-aim-card-collapsed {
        background: #f0f7ff;
        border: 2px solid var(--monash-blue);
        border-radius: 4px;
        padding: 20px;
        margin: 15px 0;
      }
      .sub-aim-number {
        background: var(--monash-blue);
        color: var(--monash-white);
        width: 40px;
        height: 40px;
        border-radius: 50%;
        display: inline-flex;
        align-items: center;
        justify-content: center;
        font-weight: bold;
        font-size: 1.2em;
        margin-right: 15px;
        vertical-align: top;
      }
      .sub-aim-header {
        display: flex;
        align-items: flex-start;
        gap: 15px;
        margin-bottom: 12px;
      }
      .sub-aim-title {
        color: var(--monash-black);
        font-weight: 600;
        font-size: 1.05em;
        flex: 1;
      }
      .sub-aim-text {
        color: var(--monash-black);
        font-size: 0.95em;
        line-height: 1.6;
      }

      .reference-images {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
        background: #fafafa;
        padding: 20px;
        border-radius: 4px;
        border: 1px solid #ddd;
      }
      .image-card {
        text-align: center;
        background: var(--monash-white);
        padding: 15px;
        border-radius: 4px;
        box-shadow: 0 1px 4px rgba(0, 0, 0, 0.1);
      }
      .image-card img {
        max-width: 100%;
        height: auto;
        border-radius: 3px;
        margin-bottom: 12px;
        border: 1px solid #ddd;
      }
      .image-caption {
        font-size: 0.9em;
        color: var(--monash-black);
        line-height: 1.5;
        text-align: left;
      }
      .image-label {
        font-weight: 700;
        color: var(--monash-blue);
        margin-bottom: 5px;
        font-size: 0.95em;
      }
      .image-details {
        font-size: 0.85em;
        color: var(--monash-grey);
        margin-top: 8px;
        font-style: italic;
      }

      ul,
      ol {
        margin-left: 20px;
        line-height: 1.8;
      }
      li {
        margin-bottom: 8px;
      }

      /* Missing CSS for Slides 4-8 */
      .model-specs-grid {
        display: grid;
        grid-template-columns: 1fr 1fr;
        gap: 20px;
        margin: 30px 0;
      }
      .model-spec-card {
        padding: 25px;
        border-radius: 4px;
        text-align: center;
        box-shadow: 0 2px 6px rgba(0, 0, 0, 0.1);
      }
      .model-spec-card.lightweight {
        background: #e8f5e9;
        border: 2px solid #00a651;
      }
      .model-spec-card.heavy {
        background: #ffebee;
        border: 2px solid #d32f2f;
      }
      .spec-label {
        font-size: 1.2em;
        font-weight: 700;
        margin-bottom: 10px;
        color: var(--monash-black);
      }
      .spec-value {
        font-size: 2.5em;
        font-weight: 700;
        margin: 15px 0;
        font-family: "Roboto Condensed", sans-serif;
        color: var(--monash-blue);
      }
      .spec-detail {
        font-size: 0.9em;
        line-height: 1.6;
        color: var(--monash-black);
      }
      .critical-box {
        background: #ffebee;
        border-left: 4px solid #d32f2f;
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .critical-title {
        color: #c62828;
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .critical-text {
        color: var(--monash-black);
        line-height: 1.6;
      }
      .infrastructure-box {
        background: #f3f4f6;
        border-left: 4px solid var(--monash-grey);
        padding: 20px;
        margin: 20px 0;
        border-radius: 2px;
      }
      .infrastructure-title {
        color: var(--monash-black);
        font-weight: 700;
        font-size: 1.05em;
        margin-bottom: 10px;
      }
      .infrastructure-text {
        color: var(--monash-black);
        line-height: 1.6;
      }
      .optimal-recommendation-box {
        background: #e8f5e9;
        border: 3px solid #00a651;
        padding: 25px;
        margin: 20px 0;
        border-radius: 4px;
      }
      .optimal-title {
        color: #006b3f;
        font-weight: 700;
        font-size: 1.15em;
        margin-bottom: 15px;
      }
      .optimal-text {
        color: var(--monash-black);
        line-height: 1.7;
      }
      .comparison-table .optimal {
        background: #e8f5e9;
        color: #006b3f;
        font-weight: 600;
      }
      .comparison-table .critical {
        background: #ffebee;
        color: #c62828;
        font-weight: 600;
      }
    </style>
  </head>
  <body>
    <div class="presentation">
      <!-- COVER SLIDE -->
      <div class="slide cover-slide">
        <div class="logo-header">
          <div class="monash-logo-top">
            <img src="./monash-logo.png" alt="Monash University Logo" />
          </div>
          <div class="soda-labs-logo-top">
            <img src="./soda-labs-logo.jpeg" alt="SoDa Labs Logo" />
          </div>
        </div>

        <div style="text-align: center; margin-top: 40px">
          <h1>Vision Language Models for Historical Document Digitization</h1>
          <p class="subtitle">
            Benchmarking Qwen3-VL Against Traditional OCR for Austrian Parish
            Records
          </p>
          <div class="author-info">
            <p><strong>Jestin Roy</strong></p>
            <p>Research Assistant, Department of Economics</p>
            <p>Master of Data Science Student</p>
            <p>Monash Business School, Monash University</p>
            <p style="margin-top: 20px; opacity: 0.85">
              June 2025 - Present | Winter Research Scholarship Program
            </p>
          </div>
          <div class="soda-labs-badge">üöÄ SoDa Labs Research Initiative</div>
        </div>
      </div>

      <!-- SLIDE 1: Research Background & Motivation -->
      <div class="slide">
        <h1>Research Background & Motivation</h1>

        <h2>Personal Introduction</h2>
        <div class="intro-item"><strong>Name:</strong> Jestin Roy</div>
        <div class="intro-item">
          <strong>Position:</strong> Research Assistant (RA) under Professor
          Paul, Department of Economics
        </div>
        <div class="intro-item">
          <strong>Education:</strong> Master of Data Science student
        </div>
        <div class="intro-item">
          <strong>Experience:</strong> AI Engineer & Full-Stack Developer (2
          years industrial experience)
        </div>
        <div class="intro-item">
          <strong>Project Duration:</strong> June 2025 - Present (Winter
          Research Scholarship Program)
        </div>

        <h2>The Research Challenge</h2>
        <div class="intro-item">
          <strong>Documents:</strong> Austrian parish records from 1800-1920s
          (Sterbb√ºcher [death registers], Taufe B√ºcher [baptism records])
        </div>
        <div class="intro-item">
          <strong>Script Type:</strong> S√ºtterlin/Kurrent script (historical
          German cursive and Gothic scripts)
        </div>
        <div class="intro-item">
          <strong>Research Goal:</strong> Accurate digitization for historical
          scholarship, genealogical research, and cultural heritage preservation
        </div>

        <h2>Why S√ºtterlin/Kurrent Scripts Are Challenging</h2>
        <ul>
          <li>
            <strong>S√ºtterlin (1911+):</strong> German Gothic cursive with sharp
            angles and connected letterforms
          </li>
          <li>
            <strong>Kurrent (1600s-1900s):</strong> Historical German script
            with unique character forms (≈ø, √§, √∂, √º variations)
          </li>
          <li>
            <strong>Visual Complexity:</strong> Ligatures, abbreviations, and
            archaic letterforms unfamiliar to standard ML models
          </li>
          <li>
            <strong>Training Data Gap:</strong> Modern OCR models trained on
            20th-21st century text, not 19th-century German scripts
          </li>
        </ul>

        <h2>The Problem: Predecessor Approach (trOCR + Danish Fine-Tuning)</h2>
        <div class="discovery-box">
          <div class="discovery-title">Why Character-Level OCR Failed</div>
          <div class="discovery-text">
            ‚ùå Language Mismatch: Trained on Danish, applied to German
            S√ºtterlin/Kurrent<br />
            ‚ùå Script Mismatch: Danish modern text ‚â† 19th-century Austrian
            Gothic scripts<br />
            ‚ùå No Visual Reasoning: Character-by-character OCR cannot understand
            document context<br />
            ‚ùå No Layout Understanding: Cannot distinguish table structure from
            continuous text<br />
            ‚ùå Limited Transfer: Extensive fine-tuning required for each new
            language/script
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">
            <div class="footer-logo">
              <img src="./monash-logo.png" alt="Monash Logo" />
            </div>
            Monash Business School
          </div>
          <div class="footer-right">Slide 1 of 7</div>
        </div>
      </div>

      <!-- SLIDE 2: Methodological Breakthrough -->
      <div class="slide page-break">
        <h1>The Methodological Breakthrough</h1>

        <h2>Three Phases of the Research</h2>

        <div class="phase-timeline">
          <div class="phase">
            <div class="phase-circle">1</div>
            <div class="phase-title">Initial Baseline</div>
            <div class="phase-text">
              trOCR (Danish fine-tuning)<br />Poor generalization
            </div>
          </div>
          <div class="phase">
            <div class="phase-circle">2</div>
            <div class="phase-title">Ground Truth Creation</div>
            <div class="phase-text">
              Closed-Source LLMs (GPT-4.1)<br />Visual reasoning
            </div>
          </div>
          <div class="phase">
            <div class="phase-circle">3</div>
            <div class="phase-title">VLM Evaluation</div>
            <div class="phase-text">Qwen3-VL vs Chandra<br />Benchmarking</div>
          </div>
        </div>

        <h2>Why Closed-Source LLMs for Ground Truth?</h2>

        <table class="comparison-table">
          <tr>
            <th>Capability</th>
            <th>Fine-Tuning Approach</th>
            <th>Ground Truth Approach</th>
          </tr>
          <tr>
            <td><strong>Visual Reasoning</strong></td>
            <td class="cross">‚úó None (character-level only)</td>
            <td class="checkmark">‚úì Yes (holistic understanding)</td>
          </tr>
          <tr>
            <td><strong>Handwriting Context</strong></td>
            <td class="cross">‚úó Limited (language-specific)</td>
            <td class="checkmark">‚úì Full (visual + semantic)</td>
          </tr>
          <tr>
            <td><strong>Domain Adaptation Time</strong></td>
            <td>2-3 months</td>
            <td>Minutes</td>
          </tr>
          <tr>
            <td><strong>Accuracy on Historical Scripts</strong></td>
            <td>50-70%</td>
            <td>95%+ (human-level)</td>
          </tr>
          <tr>
            <td><strong>Cost to Scale</strong></td>
            <td>High (annotation labor)</td>
            <td>Low (same model)</td>
          </tr>
          <tr>
            <td><strong>Generalization Ability</strong></td>
            <td>Poor (language-specific)</td>
            <td>Excellent (cross-language)</td>
          </tr>
        </table>

        <h2>Current Investigation</h2>
        <div class="recommendation-box">
          <div class="recommendation-title">Research Question</div>
          <div class="recommendation-text">
            Can we achieve closed-source LLM performance using open-source
            Vision Language Models (VLMs) at lower cost and without fine-tuning?
          </div>
        </div>

        <h2>Models Being Evaluated</h2>
        <ul>
          <li>
            <strong>Qwen3-VL-32B:</strong> Open-source Vision Language Model
            trained on billions of image-text pairs
          </li>
          <li>
            <strong>Chandra OCR:</strong> Traditional specialized OCR engine
            (character detection and recognition)
          </li>
          <li>
            <strong>Against Ground Truth:</strong> GPT-4.1 and Gemini 2.5 Pro
            (visual reasoning, context understanding)
          </li>
          <li>
            <strong>Dataset:</strong> Austrian parish records (S√ºtterlin/Kurrent
            scripts, 1643-1920)
          </li>
        </ul>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 2 of 7</div>
        </div>
      </div>

      <!-- SLIDE 3: Benchmark Results with Reference Images -->
      <div class="slide page-break">
        <h1>Benchmark Results - Performance Metrics</h1>

        <h2>Reference Images: Proof-of-Concept Test Documents</h2>
        <div class="reference-images">
          <div class="image-card">
            <img
              src="./Althofen_Sterbbuch_TomVII_1905-1920_page_006.jpg"
              alt="Modern Austrian Sterbbuch 1905-1920"
            />
            <div class="image-caption">
              <div class="image-label">
                Althofen_Sterbbuch_TomVII_1905-1920_page_006.jpg
              </div>
              <strong>Era:</strong> Modern (1905-1920)<br />
              <strong>Type:</strong> Sterbbuch (Death Register)<br />
              <strong>Performance:</strong> 81.27% Word F1 ‚úì
              <div class="image-details">Production-Ready Accuracy</div>
            </div>
          </div>

          <div class="image-card">
            <img
              src="./Wein_Neiderosterich_MariaBrunn_1887_TaufeBuch_0012.jpg"
              alt="Late 19th Century Austrian Taufebuch 1887"
            />
            <div class="image-caption">
              <div class="image-label">
                Wein_Neiderosterich_MariaBrunn_1887_TaufeBuch_0012.jpg
              </div>
              <strong>Era:</strong> Late 19th Century (1887)<br />
              <strong>Type:</strong> Taufebuch (Baptism Register)<br />
              <strong>Performance:</strong> 61.54% Word F1 ‚ö†
              <div class="image-details">Usable + Manual Verification</div>
            </div>
          </div>

          <div class="image-card">
            <img
              src="./Vienna_1783_StPeter_TaufeBuch_0017.jpg"
              alt="18th Century Viennese Taufebuch 1783"
            />
            <div class="image-caption">
              <div class="image-label">
                Vienna_1783_StPeter_TaufeBuch_0017.jpg
              </div>
              <strong>Era:</strong> 18th Century (1783)<br />
              <strong>Type:</strong> Taufebuch (Baptism Register)<br />
              <strong>Performance:</strong> 37.01% Word F1 ‚ö†
              <div class="image-details">
                UTF-8 Fix: 0.92% ‚Üí 37.01% (40x improvement)
              </div>
            </div>
          </div>

          <div class="image-card">
            <img
              src="./Althofen_SterbBuch_1643_A05_022-1_00007.jpg"
              alt="17th Century Pure Kurrent Script 1643"
            />
            <div class="image-caption">
              <div class="image-label">
                Althofen_SterbBuch_1643_A05_022-1_00007.jpg
              </div>
              <strong>Era:</strong> 17th Century (1643)<br />
              <strong>Type:</strong> Sterbbuch (Death Register)<br />
              <strong>Performance:</strong> 11.62% Word F1 ‚úó
              <div class="image-details">
                Pure Kurrent - Not Viable Without Specialized Training
              </div>
            </div>
          </div>
        </div>

        <h2>Overall Performance Comparison</h2>

        <div class="metrics-grid">
          <div class="metric-card">
            <div class="metric-label">Word F1 Score</div>
            <div class="metric-value">84.96%</div>
            <div class="metric-label">Qwen3-VL</div>
            <div class="advantage">+4.44% vs Chandra</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Word Recall</div>
            <div class="metric-value">86.81%</div>
            <div class="metric-label">Qwen3-VL</div>
            <div class="advantage">+8.61% vs Chandra</div>
          </div>
          <div class="metric-card">
            <div class="metric-label">Character F1</div>
            <div class="metric-value">97.49%</div>
            <div class="metric-label">Qwen3-VL</div>
            <div class="advantage">+0.90% vs Chandra</div>
          </div>
        </div>

        <h2>Key Insight: The Character-Layout Gap</h2>
        <div class="discovery-box">
          <div class="discovery-title">Critical Finding</div>
          <div class="discovery-text">
            Both models achieve 97% character-level accuracy, but only 85%
            word-level accuracy.<br /><br />
            <strong>Implication:</strong> Layout understanding (not character
            recognition) is the primary bottleneck.<br />
            Models successfully read individual characters but struggle to
            preserve word boundaries and table structure.
          </div>
        </div>

        <h2>Performance by Document Era (Temporal Trend)</h2>

        <table class="comparison-table">
          <tr>
            <th>Era</th>
            <th>Period</th>
            <th>Qwen3-VL Word F1</th>
            <th>Status</th>
          </tr>
          <tr>
            <td><strong>Modern</strong></td>
            <td>1905-1920</td>
            <td>81.27%</td>
            <td class="checkmark">‚úì Production-Ready</td>
          </tr>
          <tr>
            <td><strong>Late 19th Century</strong></td>
            <td>1887</td>
            <td>61.54%</td>
            <td>‚ö† Usable + Verification</td>
          </tr>
          <tr>
            <td><strong>18th Century</strong></td>
            <td>1783</td>
            <td>37.01%</td>
            <td>‚ö† High Manual Effort</td>
          </tr>
          <tr>
            <td><strong>17th Century</strong></td>
            <td>1643</td>
            <td>11.62%</td>
            <td class="cross">‚úó Not Viable</td>
          </tr>
        </table>

        <h2>Critical Discovery: UTF-8 Encoding Impact</h2>
        <div class="discovery-box">
          <div class="discovery-title">
            Vienna1783 Case Study - 40x Improvement
          </div>
          <div class="discovery-text">
            <strong>Before proper UTF-8 encoding:</strong> 0.92% Word F1
            (catastrophic)<br />
            <strong>After UTF-8 fix:</strong> 37.01% Word F1 (restored
            usability)<br /><br />
            <strong>Lesson:</strong> Data quality validation is as important as
            model selection. Character encoding errors can destroy benchmark
            reliability.
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 3 of 7</div>
        </div>
      </div>

      <!-- SLIDE 4: Model Infrastructure Requirements - CORRECTED -->
      <div class="slide page-break">
        <h1>Infrastructure & Resource Constraints: The Critical Factor</h1>

        <h2>Model Size & Memory Requirements</h2>
        <div class="model-specs-grid">
          <div class="model-spec-card lightweight">
            <div class="spec-label">Chandra OCR</div>
            <div class="spec-value">7B</div>
            <div class="spec-detail">
              Parameters (lightweight)<br /><br />Memory: 19 GB per instance<br />Can
              parallelize 10x on Nectar<br />Total RAM needed: ~190 GB<br /><strong
                class="checkmark"
                >‚úì Well within server capacity</strong
              >
            </div>
          </div>
          <div class="model-spec-card heavy">
            <div class="spec-label">Qwen3-VL</div>
            <div class="spec-value">32B</div>
            <div class="spec-detail">
              Parameters (very heavy)<br /><br />Memory: 60+ GB per instance<br />Cannot
              safely parallelize<br />10x instances = 600+ GB RAM<br /><strong
                class="critical"
                >‚úó EXCEEDS A100 GPU capacity</strong
              >
            </div>
          </div>
        </div>

        <h2>Why Large VLMs Cause System Collapse</h2>
        <div class="critical-box">
          <div class="critical-title">
            ‚ö†Ô∏è Infrastructure Risk: Qwen3-VL Parallelization Hazard
          </div>
          <div class="critical-text">
            <strong>Problem:</strong> Qwen3-VL's 32B parameters = 60+ GB memory
            footprint per instance<br /><br />
            <strong>Parallelization Attempt (10 workers):</strong><br />
            ‚Ä¢ Memory needed: ~600+ GB VRAM<br />
            ‚Ä¢ Even latest A100 GPUs: Max ~320 GB cluster capacity<br />
            ‚Ä¢ Result: Server memory overflow ‚Üí Kernel panic ‚Üí System crash<br /><br />
            <strong>Production Impact:</strong> Cannot achieve HPC
            parallelization benefits. Single-instance processing returns us to
            the original 13-minute bottleneck. Process 10,000 images in ~90
            days, not 2 days.
          </div>
        </div>

        <h2>Chandra OCR: Infrastructure-Efficient Solution</h2>

        <table class="comparison-table">
          <tr>
            <th>Metric</th>
            <th>Chandra OCR</th>
            <th>Qwen3-VL</th>
          </tr>
          <tr>
            <td><strong>Model Size</strong></td>
            <td class="optimal">7B parameters</td>
            <td class="critical">32B parameters</td>
          </tr>
          <tr>
            <td><strong>Memory per Instance</strong></td>
            <td class="optimal">19 GB</td>
            <td class="critical">60+ GB</td>
          </tr>
          <tr>
            <td><strong>Safe Parallelization (10 workers)</strong></td>
            <td class="optimal">190 GB total ‚úì</td>
            <td class="critical">600+ GB total ‚úó</td>
          </tr>
          <tr>
            <td><strong>A100 GPU Cluster Capacity</strong></td>
            <td class="optimal">~320 GB available</td>
            <td class="critical">Insufficient</td>
          </tr>
          <tr>
            <td><strong>Crash Risk with Parallelization</strong></td>
            <td class="optimal">Safe margin</td>
            <td class="critical">HIGH - Server collapse</td>
          </tr>
          <tr>
            <td><strong>Processing 10,000 images</strong></td>
            <td class="optimal">~2 days (parallelized)</td>
            <td class="critical">~90 days (single instance)</td>
          </tr>
          <tr>
            <td><strong>Accuracy Trade-Off</strong></td>
            <td>80.52% F1</td>
            <td>84.96% F1</td>
          </tr>
        </table>

        <div class="infrastructure-box">
          <div class="infrastructure-title">
            üñ•Ô∏è Nectar HPC Infrastructure Constraints
          </div>
          <div class="infrastructure-text">
            <strong>Available Resources:</strong><br />
            ‚Ä¢ GPU Cluster: Multiple A100 GPUs with ~320 GB combined VRAM<br />
            ‚Ä¢ CPU RAM: Limited (<400 GB per node)<br />
            ‚Ä¢ Network bandwidth: 100 Gbps (sufficient for I/O)<br /><br />
            <strong>Design Constraint:</strong> Qwen3-VL's 60+ GB footprint
            eliminates parallelization. Chandra's 19 GB enables 10x parallel
            workers safely.
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 4 of 8</div>
        </div>
      </div>

      <!-- SLIDE 5: Computational Trade-Offs -->
      <div class="slide page-break">
        <h1>Computational Trade-Off Analysis</h1>

        <h2>Processing Time Comparison</h2>
        <div class="time-comparison-grid">
          <div class="time-card fast">
            <div class="time-label">Chandra OCR</div>
            <div class="time-value">2-3 min</div>
            <div class="time-detail">per test image<br />(Single run)</div>
          </div>
          <div class="time-card slow">
            <div class="time-label">Qwen3-VL-32B</div>
            <div class="time-value">13 min</div>
            <div class="time-detail">per test image<br />(Single run)</div>
          </div>
        </div>

        <h2>Scalability Impact Analysis</h2>

        <table class="comparison-table">
          <tr>
            <th>Scenario</th>
            <th>Chandra (Parallelized)</th>
            <th>Qwen3-VL (Single)</th>
          </tr>
          <tr>
            <td><strong>1000 Images</strong></td>
            <td class="optimal">~5 hours</td>
            <td>~216 hours</td>
          </tr>
          <tr>
            <td><strong>10,000 Images</strong></td>
            <td class="optimal">~2 days</td>
            <td>~90 days</td>
          </tr>
          <tr>
            <td><strong>100,000 Images</strong></td>
            <td class="optimal">~20 days</td>
            <td>~900 days</td>
          </tr>
          <tr>
            <td><strong>Accuracy</strong></td>
            <td>80.52% F1</td>
            <td>84.96% F1</td>
          </tr>
          <tr>
            <td><strong>Accuracy Trade-Off</strong></td>
            <td class="optimal">4.44% loss</td>
            <td>Baseline</td>
          </tr>
          <tr>
            <td><strong>Parallelization Feasible</strong></td>
            <td class="optimal">‚úì YES (safe)</td>
            <td class="critical">‚úó NO (crash risk)</td>
          </tr>
        </table>

        <h2>Why Chandra is the Production Winner</h2>
        <div class="discovery-box">
          <div class="discovery-title">
            The Case for Chandra OCR (Infrastructure + Accuracy Trade-Off)
          </div>
          <div class="discovery-text">
            ‚úì <strong>Already Beats Most Models:</strong> Chandra's 80.52% F1
            outperforms nearly all traditional OCR engines<br />
            ‚úì <strong>Qwen3-VL Marginal Gain:</strong> Only +4.44% improvement,
            NOT worth infrastructure risk<br />
            ‚úì <strong>Infrastructure Safety:</strong> 7B parameters vs 32B =
            lightweight, enables safe 10x parallelization<br />
            ‚úì <strong>Server Stability:</strong> Chandra won't crash Nectar
            cluster; Qwen3-VL WILL with parallelization<br />
            ‚úì <strong>Throughput Multiplier:</strong> 40x faster with
            parallelization (5 hours vs 216 hours for 1000 images)
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 5 of 8</div>
        </div>
      </div>

      <!-- SLIDE 6: Key Discoveries & Recommendations -->
      <div class="slide page-break">
        <h1>Key Discoveries & Optimal Recommendations</h1>

        <h2>Research Question & Answer</h2>
        <div class="conclusion-highlight">
          <strong
            >Q: Can Vision Language Models outperform traditional OCR on
            historical Austrian S√ºtterlin/Kurrent manuscripts without
            fine-tuning?</strong
          ><br /><br />
          <strong>A: YES, but NOT for production deployment</strong><br /><br />
          Qwen3-VL achieves 84.96% Word F1 (+4.44% vs Chandra), yet
          infrastructure constraints prevent safe parallelization on Nectar HPC.
          Chandra's lightweight 19 GB footprint enables 10x parallelization
          without server collapse.
        </div>

        <h2>Four Key Discoveries</h2>

        <div class="discovery-box">
          <div class="discovery-title">1. Character Recognition is Solved</div>
          <div class="discovery-text">
            Both Qwen3-VL (97.49%) and Chandra (96.59%) achieve near-perfect
            character-level accuracy. Investment in character recognition has
            diminishing returns. Focus should shift to layout understanding.
          </div>
        </div>

        <div class="discovery-box">
          <div class="discovery-title">2. Document Age is the Key Driver</div>
          <div class="discovery-text">
            Performance directly correlates with document era: Modern (81%) ‚Üí
            19th century (61%) ‚Üí 18th century (37%) ‚Üí 17th century (12%). Pure
            Kurrent/Fraktur scripts require specialized training.
          </div>
        </div>

        <div class="discovery-box">
          <div class="discovery-title">
            3. Infrastructure Trumps Accuracy in Production
          </div>
          <div class="discovery-text">
            Qwen3-VL's 32B parameters consume 60+ GB per instance. Parallelizing
            10x would exceed Nectar cluster capacity (~320 GB VRAM), causing
            system crash. Chandra's 7B parameters at 19 GB per instance enable
            safe 10x parallelization.
          </div>
        </div>

        <div class="discovery-box">
          <div class="discovery-title">
            4. Chandra OCR is the Production-Optimal Choice
          </div>
          <div class="discovery-text">
            4% accuracy loss (80.52% vs 84.96%) is negligible compared to 40x
            throughput gain with safe parallelization. For 10,000-image
            archives: 88 days saved, infrastructure stability maintained,
            realistic production deployment.
          </div>
        </div>

        <h2>Immediate Recommendations (Next 3-6 Months)</h2>

        <div class="optimal-recommendation-box">
          <div class="optimal-title">
            ‚≠ê PRIMARY RECOMMENDATION: Deploy Chandra OCR with Safe HPC
            Parallelization
          </div>
          <div class="optimal-text">
            <strong>Strategy:</strong> Use Chandra OCR as the primary production
            model with 10x GPU workers on Nectar HPC cluster for parallel batch
            processing. Lightweight architecture ensures server stability while
            achieving 40x throughput improvement.<br /><br />
            <strong>Implementation:</strong><br />
            ‚úì Deploy 10x parallel GPU workers on Nectar (each running 19 GB
            Chandra instance)<br />
            ‚úì Total memory footprint: ~190 GB (safe within 320 GB cluster
            capacity)<br />
            ‚úì Process documents in batches (10 images simultaneously)<br />
            ‚úì Expected accuracy: 80.52% Word F1 (acceptable for most
            archives)<br />
            ‚úì Effective throughput: Process 1,000 images in ~5 hours (40x
            improvement vs 216 hours serial)<br /><br />
            <strong>Result for 10,000-image archive:</strong><br />
            ‚Ä¢ Chandra parallelized: ~2 days (vs. 90 days Qwen3-VL serial)<br />
            ‚Ä¢ Server stability: ‚úì Confirmed (stays within infrastructure
            limits)<br />
            ‚Ä¢ Accuracy sacrifice: 4.44% (negligible for large-scale
            digitization)
          </div>
        </div>

        <div class="recommendation-box">
          <div class="recommendation-title">
            SECONDARY RECOMMENDATION: Reserve Qwen3-VL for Research Only
          </div>
          <div class="recommendation-text">
            ‚úì Use Qwen3-VL only for research/proof-of-concept studies<br />
            ‚úì Single-instance deployment only (never attempt parallelization)<br />
            ‚úì Apply to small subset of documents where 4% accuracy gain is
            critical<br />
            ‚úì Do NOT attempt to scale Qwen3-VL on production infrastructure
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 6 of 8</div>
        </div>
      </div>

      <!-- SLIDE 7: Broader Research Context -->
      <div class="slide page-break">
        <h1>Broader Research Context & Project Aims</h1>

        <h2>The Bigger Picture: Historical Economic & Demographic Analysis</h2>
        <div class="project-overview">
          <h3>Project Aim</h3>
          <div class="overview-item">
            <span class="overview-label">Goal:</span> Perform layout analysis
            and handwritten text recognition (HTR) on Austrian parish records,
            extract tabular data, identify key figures from regions, and track
            their descendants' economic progress across decades (1800-1920s,
            including World War I and beyond).
          </div>
          <div class="overview-item">
            <span class="overview-label">Historical Context:</span> Tracing
            families' economic trajectories during transformative periods in
            Austrian history
          </div>
        </div>

        <h2>The Six-Stage Pipeline</h2>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">1</div>
            <div class="sub-aim-title">
              Ground Truth Creation: Hand-Transcribe Austrian Birth Certificates
            </div>
          </div>
          <div class="sub-aim-text">
            Using Transkribus desktop app, transcribe 496 word-level images from
            Austrian birth certificates. Dataset manually refined by Prof. Paul
            using PyLaia HTR model (German_Kurrent_17th-18th, CER: 6.00% train /
            5.50% validation).
          </div>
        </div>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">2-4</div>
            <div class="sub-aim-title">
              HTR Model Training & Testing ‚Üí Entity Extraction
            </div>
          </div>
          <div class="sub-aim-text">
            <strong>Sub-aim 2:</strong> Train HTR model (TrOCR) on combined
            open-source HANA + hand-transcribed Austrian data.<br />
            <strong>Sub-aim 3:</strong> Test trained model on complete Austrian
            birth dataset, producing structured machine-readable
            transcriptions.<br />
            <strong>Sub-aim 4:</strong> Apply Named Entity Recognition (NER) to
            extract names, professions, and place names from HTR outputs,
            enabling demographic mapping and occupational analysis.
          </div>
        </div>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">5</div>
            <div class="sub-aim-title">
              Construct Historical Social Networks
            </div>
          </div>
          <div class="sub-aim-text">
            Map relationships (family ties, godparent relationships, community
            associations) from birth, marriage, and death registers. Create
            connection graphs revealing social connectivity patterns and
            enabling lineage tracing across generations.
          </div>
        </div>

        <div class="sub-aim-card-collapsed">
          <div class="sub-aim-header">
            <div class="sub-aim-number">6</div>
            <div class="sub-aim-title">Demographic & Longitudinal Analysis</div>
          </div>
          <div class="sub-aim-text">
            Conduct demographic analysis examining clusters by profession,
            geography, and social connectivity. Link to historical events (WWI,
            radical movements) to trace economic progress, social status shifts,
            and the impact of historical events on community development.
          </div>
        </div>

        <h2>How Chandra OCR Enables the Pipeline (Production-Ready)</h2>
        <div class="conclusion-highlight">
          Chandra OCR's 80.52% Word F1 with high throughput directly enables
          rapid transcriptions in Sub-aims 2-3. With safe 10x parallelization on
          Nectar HPC (19 GB footprint), digitize 10,000 documents in ~2 days,
          creating the foundation for all downstream entity extraction (Sub-aim
          4), network construction (Sub-aim 5), and historical analysis (Sub-aim
          6).
        </div>

        <div class="footer">
          <div class="footer-left">Monash Business School</div>
          <div class="footer-right">Slide 7 of 8</div>
        </div>
      </div>

      <!-- CONCLUSION SLIDE -->
      <div class="slide page-break">
        <h1>Conclusion: Infrastructure-Aware Production Strategy</h1>

        <div class="conclusion-highlight" style="margin-top: 30px">
          <strong>Research Finding:</strong> Qwen3-VL achieves 84.96% accuracy
          (superior to Chandra at 80.52%)<br /><br />
          <strong>Infrastructure Reality:</strong> Qwen3-VL's 60 GB footprint
          prevents safe parallelization on Nectar cluster<br /><br />
          <strong>Production Decision:</strong> Deploy Chandra OCR (7B params,
          19 GB) with 10x HPC parallelization for 40x throughput at 4% accuracy
          cost
        </div>

        <h2 style="margin-top: 50px">Impact Summary</h2>

        <div class="discovery-box">
          <div class="discovery-title">Research Contribution</div>
          <div class="discovery-text">
            ‚úì Demonstrated VLMs outperform traditional OCR on historical
            handwriting (84.96% vs 80.52%)<br />
            ‚úì Proved ground-truth approach superior to external dataset
            fine-tuning<br />
            ‚úì Identified layout understanding as key bottleneck (not character
            recognition)<br />
            ‚úì Validated UTF-8 encoding importance in OCR pipelines<br />
            ‚úì
            <strong
              >Quantified infrastructure constraint: 60 GB (Qwen3-VL) prevents
              parallelization; 19 GB (Chandra 7B) enables 10x safe
              scaling</strong
            >
          </div>
        </div>

        <div class="recommendation-box">
          <div class="recommendation-title">
            Next Phase: Production Implementation (SoDa Labs Initiative)
          </div>
          <div class="recommendation-text">
            ‚úì Deploy Chandra OCR (7B parameters) with 10x GPU workers on Nectar
            HPC for parallel batch processing<br />
            ‚úì Process Austrian parish records (1850+) at scale: 10,000 images in
            ~2 days<br />
            ‚úì Achieve 80.52% Word F1 accuracy (acceptable for large archives)<br />
            ‚úì Maintain server stability (stay within 320 GB cluster VRAM
            capacity)<br />
            ‚úì Scale to other European archives (Czech, Polish, Hungarian
            records)<br />
            ‚úì Reserve Qwen3-VL for selective research (single-instance only)
          </div>
        </div>

        <div class="footer">
          <div class="footer-left">
            Monash Business School, Monash University
          </div>
          <div class="footer-right">Slide 8 of 8</div>
        </div>
      </div>
    </div>
  </body>
</html>

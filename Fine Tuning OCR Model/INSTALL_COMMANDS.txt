
# =============================================================================
# COMPLETE INSTALLATION COMMANDS
# =============================================================================

# -------------------------
# 1. VLLM SETUP (Recommended for all three models)
# -------------------------

# Base requirements
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121
pip install vllm>=0.6.1
pip install transformers>=4.37.0
pip install accelerate
pip install jiwer pandas pillow tqdm

# Model-specific requirements
pip install qwen-vl-utils  # For Qwen2-VL models

# Optional but recommended (skip on macOS - requires NVIDIA GPU + CUDA)
# pip install flash-attn --no-build-isolation

# Verify installation
python -c "import vllm; print(f'vLLM version: {vllm.__version__}')"

# -------------------------
# 2. OLLAMA SETUP (Easier alternative, limited to smaller models)
# -------------------------

# Install Ollama (Linux/Mac)
curl -fsSL https://ollama.com/install.sh | sh

# Install Ollama (Windows)
# Download from: https://ollama.com/download

# Python client
pip install ollama jiwer pandas pillow tqdm

# Pull available models
ollama pull llama3.2-vision
ollama pull qwen2-vl:7b

# -------------------------
# 3. MODEL DOWNLOADS (vLLM - Automatic on first run)
# -------------------------

# Optional: Pre-download to save time
huggingface-cli login  # If models require authentication

# Download specific GGUF quantization (choose ONE based on your VRAM):
# For 16GB+ VRAM (best quality):
hf download unsloth/Qwen3-VL-32B-Instruct-GGUF --include "Qwen3-VL-32B-Instruct-Q4_K_M.gguf"
# For 12-16GB VRAM (good balance):
# hf download unsloth/Qwen3-VL-32B-Instruct-GGUF --include "Qwen3-VL-32B-Instruct-Q3_K_M.gguf"
# For 8-12GB VRAM (lower quality but faster):
# hf download unsloth/Qwen3-VL-32B-Instruct-GGUF --include "Qwen3-VL-32B-Instruct-Q2_K.gguf"

# Download other models
hf download datalab-to/chandra
hf download deepseek-ai/DeepSeek-OCR
`
# -------------------------
# 4. VERIFY SETUP
# -------------------------

# Check CUDA availability
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}')"
python -c "import torch; print(f'GPU: {torch.cuda.get_device_name(0)}')"

# Check GPU memory
nvidia-smi

# Test jiwer
python -c "from jiwer import wer, cer; print('jiwer working!')"

# -------------------------
# 5. QUICK TEST
# -------------------------

# Test with DeepSeek OCR 3B (smallest model)
python quickstart_ocr_benchmark.py
